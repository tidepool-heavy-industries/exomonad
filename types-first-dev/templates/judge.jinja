You are evaluating generated Haskell code for a URL shortener service.

## Specification Given

{{ description }}

### Acceptance Criteria
{% for criterion in acceptance_criteria %}
- {{ criterion }}
{% endfor %}

---

## Generated Code

### Types/API ({{ types_file }})

```haskell
{{ types_code }}
```

### Implementation ({{ impl_file }})

```haskell
{{ impl_code }}
```

### Tests ({{ tests_file }})

```haskell
{{ tests_code }}
```

---

## Evaluation Task

Score each dimension 1-5 and provide a brief rationale (1-2 sentences).

**Scale:**
- 1 = Poor (major issues, doesn't work)
- 2 = Below average (significant gaps)
- 3 = Adequate (works, room for improvement)
- 4 = Good (solid implementation)
- 5 = Excellent (exemplary, best practices)

Evaluate these dimensions:

1. **Spec Fidelity**: Does the code implement all acceptance criteria? Are the 3 endpoints present and correct?

2. **Type Design**: Are types well-chosen? Uses newtypes where appropriate (ShortCode, OriginalUrl)? Good record design?

3. **Test Quality**: Do tests cover acceptance criteria? Property-based testing where sensible? Meaningful assertions?

4. **Impl Quality**: Clean implementation? No undefined/stubs left? Handles edge cases (invalid URLs, missing codes)?

5. **Coherence**: Do types, tests, and implementation work together? Tests actually exercise the API? No mismatches?

6. **Idiomaticity**: Follows Haskell best practices? Applicative style? Proper error handling? No anti-patterns?

Respond with JSON:

```json
{
  "spec_fidelity": {"score": N, "rationale": "..."},
  "type_design": {"score": N, "rationale": "..."},
  "test_quality": {"score": N, "rationale": "..."},
  "impl_quality": {"score": N, "rationale": "..."},
  "coherence": {"score": N, "rationale": "..."},
  "idiomaticity": {"score": N, "rationale": "..."},
  "overall_summary": "One paragraph summarizing the overall quality and key observations."
}
```
